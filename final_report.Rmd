---
title: "Data Analysis Project Final Report"
author:
- "STAT 420, Summer 2018"
- "Amod Augustin - NetID: amoda2"
- "Jeff Gerlach - NetID: gerlach5"
- "Yongwoo Noh - NetID: yongwoo3"
- "Naveen Vasu - NetID: nvasu2"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

## Price Analysis and Prediction for Melbourne Housing Market

***

### Introduction

The dataset used in this project is a compilation of publically available housing data from Melbourne, Austrailia that had been scraped weekly from the website https://www.domain.com.au/ between January 28, 2016 and March 17th 2018 and shared on the data science/machine learning competition website https://www.kaggle.com. The user that provided the dataset also cleaned the data and provided some information from another source (namely the number of bedrooms) - there are missing values that will need to be dealt with in the data for the 34,857 listings provided. 

Below is a brief description of each column in the provided CSV (where each observation is a house listed for sale in Melbourne between 01/28/2016 and 03/17/2018):
 
 * Suburb: Name of Suburb in Melbourne
 * Address: Address of house that was sold
 * Rooms: Number of rooms in house that was sold
 * Type: 
    + h - house, cottage, villa, semi, terrace
    + u - unit, duplex
    + t - townhouse
 * Price: Sale price in Australian dollars (AUD)
 * Method: 
    + PI - property passed in
    + PN - sold prior not disclosed
    + S - property sold
    + SA - sold after auction
    + SN - sold not disclosed
    + SP - property sold prior
    + SS - sold after auction price not disclosed
    + VB - vendor bid
    + W - withdrawn prior to auction
 * SellerG: Real estate agent's name
 * Date: Date house was sold
 * Distance: Distance (in km) from Melbourne's central business district
 * Postcode: Postal Code
 * Bedroom2: # of bedrooms (from a different source)
 * Bathroom: # of bathrooms
 * Car: # of spots for cars
 * Landsize: Size of property (land) in meters
 * BuildingArea: Building size in sq. meters
 * YearBuilt: Year house was built
 * CouncilArea: Governing council for the area
 * Lattitude: Latitude value
 * Longtitude: Longitude value
 * Regionname: general region (West, North, etc.)
 * Propertycount: Number of properties located in the suburb


Location of data source:

Melbourne, Australia Housing Market data from January 2016 scraped from publically available results posted every week on Domain.com.au by Tony Pino.

https://www.kaggle.com/anthonypino/melbourne-housing-market#Melbourne_housing_FULL.csv

Released Under CC BY-NC-SA 4.0:
https://creativecommons.org/licenses/by-nc-sa/4.0/

Our team would like to identify the factors which affect home prices, explore trends in the housing market over time, and compare similar house listings in different suburbs to see how location affects sale price in a localized area. With the model we derive here, we hope to provide insight into housing prices and in the future apply our findings to other markets to see if it is possible to find good deals for property purchases where you can get the most value for your money. **Our team is primarily interested in achieving the best predictions for housing prices, so accounting for model assumptions is secondary.** 

***

### Methods


##### Data Preparation
```{r, message = FALSE, warning = FALSE}
library(readr)
housing = read_csv("Melbourne_housing_FULL.csv", 
        col_types = cols(Address = col_skip(), 
        Bedroom2 = col_skip(), Date = col_skip(), 
        Lattitude = col_skip(), Longtitude = col_skip(), 
        Postcode = col_skip(), SellerG = col_skip(), 
        Suburb = col_skip()), na = "NA")


#housing = housing[!apply(housing[,c(housing$BuildingArea)] == 0, 1, FUN = any, na.rm = TRUE),]

# remove all N/As
housing = na.omit(housing)
# Remove zeroes in BuildingArea as that does not make sense (incomplete data)
housing = housing[housing$BuildingArea > 0, ]

# convert char variables
housing$Type = as.factor(housing$Type)
housing$Method = as.factor(housing$Method)
housing$CouncilArea = as.factor(housing$CouncilArea)
housing$Regionname = as.factor(housing$Regionname)
#housing$Suburb = as.factor(housing$Suburb)

# Remove outlier $9 million house
housing = housing[-c(which.max(housing$Price)), ]
# Remove property built in 1196 (most likely typo)
min(housing$YearBuilt)
housing = housing[-c(which.min(housing$YearBuilt)), ]
# Remove building area under 10 m^2 (~100 ft.)
housing = housing[housing$BuildingArea > 10, ]
# Remove properties with more than 6 car spots
housing = housing[housing$Car < 6, ]

summary(housing)
str(housing)

# Too much data to process models in a reasonable amount of time, randomly downsample to 60% of the data
set.seed(42)
percent_to_keep = 0.6
sample_size = floor(percent_to_keep * nrow(housing))
downsample_idx = sample(nrow(housing), size=sample_size)
housing = housing[downsample_idx, ]

# 60/40 train/test split of data
training_percent = 0.6
housing_trn_idx = sample(nrow(housing), floor(nrow(housing)*training_percent))
housing_trn = housing[housing_trn_idx, ]
housing_tst = housing[-housing_trn_idx, ]
```

Here we prepare the dataset for model fitting. As there are 21 variables total, we decided to run through them and eliminate the redundant or potentially not as useful ones. We removed `Address`, `Suburb`, `Latitude`, `Longitude`, and `Postcode` as this location information was already encoded in `Distance`, `CouncilArea` and `Regionname` variables. `Seller` would be very localized to Melbourne, so we decided to drop that as well. `Date` (of the sale) could potentially be useful, but for the scope of our project we decided to leave this out as well. `Bedroom2` is highly correlated with `Bedrooms` as it was the number of bedrooms pulled from another data source, so we decided to drop it and just remove any properties that did not have a `Bedrooms` value.

Next, in an attempt to further shrink the dataset size, we removed any observations with missing data in the remaining columns. Then we coerced all character variables into factors. Taking a look at the `summary` output for the remaining observations, we decided to remove some outlier values. There was a $9 million property which was not representative of the rest of the dataset, an apparent typo in a listing with a `YearBuilt` of `1196` (a probable typo), 21 listings with building size of less than 10 meters squared (~100 sq.ft), and 11 listings with more than 6 car spots - all of these observations were removed to clean the data and ease processing.

On our first trial runs to fit models, it took nearly 10 minutes to run the `R` code on the remaining data on our machines, so we made the executive decision to randomly downsample the data to `r percent_to_keep*100`% of the total size before splitting the train/test set to 60%/40% of the remaining data. This cut down the processing time to be more reasonable.
 
##### Fitting models

```{r}
# for ease of calculating LOOCV RMSE
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
# deal with log response variable
calc_loocv_rmse_log = function(model) {
  new_resid = exp(predict(model, newdata=housing_trn))- housing_trn$Price
  sqrt(mean((new_resid / (1 - hatvalues(model))) ^ 2))
}

# Here we will perform forward and backwards searches, favoring BIC
# The goal will be to select a handful of models and compare them with one another

# starting points for forward searches
n = nrow(housing_trn) # for BIC
# #1 - start with no predictors, limit search to all predictors (additive)
start_forward_model_1 = lm(Price ~ 1, data = housing_trn)
forward_bic_model_1 = step(start_forward_model_1, scope = Price ~ (Rooms + Type + Method + Distance + Bathroom + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + Regionname + Propertycount), direction = "forward", k = log(n), trace = FALSE)
summary(forward_bic_model_1)

# #2 - start with no predictors, use log of response, limit search to all predictors (additive)
start_forward_model_2 = lm(log(Price) ~ 1, data = housing_trn)
forward_bic_model_2 = step(start_forward_model_2, scope = log(Price) ~ (Rooms + Bathroom + Car + Distance + Type + Regionname + Landsize + Method + BuildingArea + YearBuilt), direction = "forward", k = log(n), trace = FALSE)
summary(forward_bic_model_2)

# #3 - start with no predictors, use log of response, limit search to 2nd-order interactions of all predictors
start_forward_model_3 = lm(log(Price) ~ 1, data = housing_trn)
forward_bic_model_3 = step(start_forward_model_3, scope = log(Price) ~ (Rooms + Bathroom + Car + Distance + Type + Regionname + Landsize + Method + BuildingArea + YearBuilt)^2, direction = "forward", k = log(n), trace = FALSE)
summary(forward_bic_model_3)



# starting points for backward searches
# #1 - start with all predictors (additive)
start_backward_model_1 = lm(Price ~ (.), data = housing_trn)
backward_bic_model_1 = step(start_backward_model_1, direction = "backward", k = log(n), trace = FALSE)
summary(backward_bic_model_1)

# #2 - start with all 2nd-order interactions - remove CouncilArea since there are 33 levels to the factor
start_backward_model_2 = lm(Price ~ (.-CouncilArea)^2, data = housing_trn)
backward_bic_model_2 = step(start_backward_model_2, direction = "backward", k = log(n), trace = FALSE)
summary(backward_bic_model_2)

# #3 - add in polynomial terms with 2nd-order interactions- remove CouncilArea since there are 33 levels to the factor
start_backward_model_3 = lm(Price ~ (.-CouncilArea)^2 + I(Distance^2) + I(Rooms^2) + I(BuildingArea^2) + I(Propertycount^2), data = housing_trn)
backward_bic_model_3 = step(start_backward_model_3, direction = "backward", k = log(n), trace = FALSE)
summary(backward_bic_model_3)

# #4 - add in polynomial and log terms with 2nd-order interactions- remove CouncilArea since there are 33 levels to the factor
start_backward_model_4 = lm(log(Price) ~ (.-CouncilArea)^2 + I(Distance^2) + I(Rooms^2) + I(BuildingArea^2) + I(Propertycount^2), data = housing_trn)
backward_bic_model_4 = step(start_backward_model_4, direction = "backward", k = log(n), trace = FALSE)
summary(backward_bic_model_4)


# Manually manipulate the search results to see if we can find a better performing model with LOOCV

#Trying some quadratic expressions based on the key predictor [found by human analysis]

manual_model_1 = lm(log(Price) ~ Rooms + Type + Bathroom + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + I(Rooms^2) + I(Landsize^2) + I(BuildingArea^2), data = housing_trn)

manual_model_2 = lm(log(Price) ~ Rooms + Type + Distance + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + Propertycount + I(Propertycount^2) + I(Rooms^2) + I(Distance^2) + I(BuildingArea^2), data = housing_trn)

manual_model_3 = lm(log(Price) ~ log(I(1/Distance)) + Car + Rooms + log(Landsize) + log(Landsize):Rooms + Type + log(I(1/Distance)):Type + Method + log(Landsize):Method + YearBuilt + log(Landsize):YearBuilt, subset = (Distance > 0) & (Landsize > 0), data=housing_trn)


# Model selection - pick best forward and backward models
# forward BIC search model LOOCV RMSE:
(fw1_loocv = calc_loocv_rmse(forward_bic_model_1))
(fw2_loocv = calc_loocv_rmse(forward_bic_model_2)) # log response
(fw3_loocv = calc_loocv_rmse(forward_bic_model_3)) # log response

# backward BIC search model LOOCV RMSE:
(bw1_loocv = calc_loocv_rmse(backward_bic_model_1))
(bw2_loocv = calc_loocv_rmse(backward_bic_model_2))
(bw3_loocv = calc_loocv_rmse(backward_bic_model_3))
(bw4_loocv = calc_loocv_rmse(backward_bic_model_4)) # log response

# manual tweaking LOOCV RMSE:
(man1_loocv = calc_loocv_rmse(manual_model_1)) # log response
(man2_loocv = calc_loocv_rmse(manual_model_2)) # log response
(man3_loocv = calc_loocv_rmse(manual_model_3)) # log response

# select the best non-log and log transformed response models since they can't be directly
# compared using LOOCV RMSE (as units are different)
which.min(c(fw1_loocv, bw1_loocv, bw2_loocv, bw3_loocv))
final_nonlog_model = backward_bic_model_3
which.min(c(fw2_loocv, fw3_loocv, bw4_loocv, man1_loocv, man2_loocv, man3_loocv))
final_log_model = backward_bic_model_4

# Probably not the most 'correct' thing to do, but we will check RMSE to the test data set for both the final models. We can then test the final performance on the non-downsampled data to ensure our final model is verified on separate data.

# log model
# re-calculate residuals so they're using the same units as the non-log model
pred_vals = exp(predict(final_log_model, newdata = housing_tst))
sqrt(mean((pred_vals - housing_tst$Price)^2))
(avg_pct_error = mean(abs((pred_vals - housing_tst$Price))/housing_tst$Price) * 100)
# non-log model
pred_vals2 = predict(final_nonlog_model, newdata = housing_tst)
sqrt(mean((pred_vals2 - housing_tst$Price)^2))
(avg_pct_error = mean(abs((pred_vals2 - housing_tst$Price))/housing_tst$Price) * 100)

# plot actual vs. predicted price values with the selected model
plot(pred_vals ~ housing_tst$Price, xlab = "Actual Values ($)", ylab = "Predicted Values ($)", main = "Predicted vs. Actual Values", col = "dodgerblue", cex = 1)
abline(0,1, col="darkorange", lwd = 1) # perfect response matches will be on this line

# plot actual vs. predicted price values with the selected model
plot(pred_vals2 ~ housing_tst$Price, xlab = "Actual Values ($)", ylab = "Predicted Values ($)", main = "Predicted vs. Actual Values", col = "dodgerblue", cex = 1)
abline(0,1, col="darkorange", lwd = 1) # perfect response matches will be on this line

# We can see that the non-log model predicts 5 houses to have negative prices...one in particular in the negative direction with a property that costs over $2 million, which drastically throws the RMSE off for the model
pred_vals2[pred_vals2 < 0]

# Thus the final winner is the log model: 
    # lm(formula = log(Price) ~ Rooms + Type + Method + Distance + 
    # Bathroom + Car + Landsize + BuildingArea + YearBuilt + Regionname + 
    # Propertycount + I(Distance^2) + I(BuildingArea^2) + I(Propertycount^2) + 
    # Rooms:Type + Rooms:Car + Rooms:BuildingArea + Type:Landsize + 
    # Type:YearBuilt + Distance:Car + Distance:Landsize + Distance:BuildingArea + 
    # Distance:Regionname + Landsize:BuildingArea + Landsize:Propertycount + 
    # BuildingArea:YearBuilt + BuildingArea:Regionname + Regionname:Propertycount, 
    # data = housing_trn)

# Test performance of the log model on the held-out data from down-sampling
remaining_test_data = housing[-downsample_idx, ]
pred_vals = exp(predict(final_log_model, newdata = remaining_test_data))
sqrt(mean((pred_vals - remaining_test_data$Price)^2))
(avg_pct_error = mean(abs((pred_vals - remaining_test_data$Price))/remaining_test_data$Price) * 100)
```

```{r}
# This analysis is secondary to the goal of prediciting housing values (vs. explaining), but still worthy of investigating in case we did want to use the model for explanation

# check selected forward, backward, and manual models
library(lmtest)
qqnorm(resid(final_log_model), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(final_log_model), col = "dodgerblue", lwd = 2)
shapiro.test(resid(final_log_model))
  
plot(fitted(final_log_model), resid(final_log_model), col = "grey", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Backward BIC Selected Log Response Model")
abline(h = 0, col = "darkorange", lwd = 2)
bptest(final_log_model)

qqnorm(resid(final_nonlog_model), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(final_nonlog_model), col = "dodgerblue", lwd = 2)
shapiro.test(resid(final_nonlog_model))
  
plot(fitted(final_nonlog_model), resid(final_nonlog_model), col = "grey", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Backward BIC Selected Non-Log Response Model")
abline(h = 0, col = "darkorange", lwd = 2)
bptest(final_nonlog_model)
```

***

### Results

***

### Discussion

After performing various search methods (forward and backwards BIC with multiple linear regression, dummy variables, interaction, transformations, and pol, along with )
```{r}
library(ggplot2)
library(ggmap)

# original data set
melbourn_housing <- read_csv("Melbourne_housing_FULL_column.csv", na = "NA")
melbourn_housing <- na.omit(melbourn_housing)
melbourn_map <- get_map(location = "melbourne", maptype = "watercolor", zoom = 11)
ggmap(melbourn_map) + geom_point(
  aes(x = Longtitude, y = Lattitude),
  colour = "red",
  alpha = 0.1,
  size = 2,
  data = melbourn_housing
  )
# training set
# test set
```

***

### Appendix