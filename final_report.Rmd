---
title: "Data Analysis Project Final Report"
author:
- "STAT 420, Summer 2018"
- "Amod Augustin - NetID: amoda2"
- "Jeff Gerlach - NetID: gerlach5"
- "Yongwoo Noh - NetID: yongwoo3"
- "Naveen Vasu - NetID: nvasu2"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

## Price Analysis and Prediction for Melbourne Housing Market

***

### Introduction

The dataset used in this project is a compilation of publically available housing data from Melbourne, Austrailia that had been scraped weekly from the website https://www.domain.com.au/ between January 28, 2016 and March 17th 2018 and shared on the data science/machine learning competition website https://www.kaggle.com. The user that provided the dataset also cleaned the data and provided some information from another source (namely the number of bedrooms) - there are missing values that will need to be dealt with in the data for the 34,857 listings provided. 

Below is a brief description of each column in the provided CSV (where each observation is a house listed for sale in Melbourne between 01/28/2016 and 03/17/2018):
 
 * Suburb: Name of Suburb in Melbourne
 * Address: Address of house that was sold
 * Rooms: Number of rooms in house that was sold
 * Type: 
    + h - house, cottage, villa, semi, terrace
    + u - unit, duplex
    + t - townhouse
 * Price: Sale price in Australian dollars (AUD)
 * Method: 
    + PI - property passed in
    + PN - sold prior not disclosed
    + S - property sold
    + SA - sold after auction
    + SN - sold not disclosed
    + SP - property sold prior
    + SS - sold after auction price not disclosed
    + VB - vendor bid
    + W - withdrawn prior to auction
 * SellerG: Real estate agent's name
 * Date: Date house was sold
 * Distance: Distance (in km) from Melbourne's central business district
 * Postcode: Postal Code
 * Bedroom2: # of bedrooms (from a different source)
 * Bathroom: # of bathrooms
 * Car: # of spots for cars
 * Landsize: Size of property (land) in meters
 * BuildingArea: Building size in sq. meters
 * YearBuilt: Year house was built
 * CouncilArea: Governing council for the area
 * Lattitude: Latitude value
 * Longtitude: Longitude value
 * Regionname: general region (West, North, etc.)
 * Propertycount: Number of properties located in the suburb


Location of data source:

Melbourne, Australia Housing Market data from January 2016 scraped from publically available results posted every week on Domain.com.au by Tony Pino.

https://www.kaggle.com/anthonypino/melbourne-housing-market#Melbourne_housing_FULL.csv

Released Under CC BY-NC-SA 4.0:
https://creativecommons.org/licenses/by-nc-sa/4.0/

Our team would like to identify the factors which affect home prices, explore trends in the housing market over time, and compare similar house listings in different suburbs to see how location affects sale price in a localized area. With the model we derive here, we hope to provide insight into housing prices and in the future apply our findings to other markets to see if it is possible to find good deals for property purchases where you can get the most value for your money. **Our team is primarily interested in achieving the best predictions for housing prices, so accounting for model assumptions is secondary.** 

***

### Methods


##### Data Preparation
```{r, message = FALSE, warning = FALSE}
library(readr)
housing = read_csv("Melbourne_housing_FULL.csv", 
        col_types = cols(Address = col_skip(), 
        Bedroom2 = col_skip(), Date = col_skip(), 
        Lattitude = col_skip(), Longtitude = col_skip(), 
        Postcode = col_skip(), SellerG = col_skip(), 
        Suburb = col_skip()), na = "NA")

# remove all N/As
housing = na.omit(housing)
# Remove zeroes in BuildingArea as that does not make sense (incomplete data)
housing = housing[housing$BuildingArea > 0, ]

# convert char variables
housing$Type = as.factor(housing$Type)
housing$Method = as.factor(housing$Method)
housing$CouncilArea = as.factor(housing$CouncilArea)
housing$Regionname = as.factor(housing$Regionname)
#housing$Suburb = as.factor(housing$Suburb)

# Remove outlier $9 million house
housing = housing[-c(which.max(housing$Price)), ]
# Remove property built in 1196 (most likely typo)
min(housing$YearBuilt)
housing = housing[-c(which.min(housing$YearBuilt)), ]
# Remove building area under 10 m^2 (~100 ft.)
housing = housing[housing$BuildingArea > 10, ]
# Remove properties with more than 6 car spots
housing = housing[housing$Car < 6, ]

summary(housing)
str(housing)

# Too much data to process models in a reasonable amount of time, randomly downsample to 60% of the data
set.seed(42)
percent_to_keep = 0.6
sample_size = floor(percent_to_keep * nrow(housing))
downsample_idx = sample(nrow(housing), size=sample_size)
housing = housing[downsample_idx, ]

# 60/40 train/test split of data
training_percent = 0.6
housing_trn_idx = sample(nrow(housing), floor(nrow(housing)*training_percent))
housing_trn = housing[housing_trn_idx, ]
housing_tst = housing[-housing_trn_idx, ]
```

 - Here we prepare the dataset for model fitting. As there are 21 variables total, we decided to run through them and eliminate the redundant or potentially not as useful ones. We removed `Address`, `Suburb`, `Latitude`, `Longitude`, and `Postcode` as this location information was already encoded in `Distance`, `CouncilArea` and `Regionname` variables. `Seller` would be very localized to Melbourne, so we decided to drop that as well. `Date` (of the sale) could potentially be useful, but for the scope of our project we decided to leave this out as well. `Bedroom2` is highly correlated with `Bedrooms` as it was the number of bedrooms pulled from another data source, so we decided to drop it and just remove any properties that did not have a `Bedrooms` value.

 - Next, in an attempt to further shrink the dataset size, we removed any observations with missing data in the remaining columns. Then we coerced all character variables into factors. Taking a look at the `summary` output for the remaining observations, we decided to remove some outlier values. There was a $9 million property which was not representative of the rest of the dataset, an apparent typo in a listing with a `YearBuilt` of `1196` (a probable typo), 21 listings with building size of less than 10 meters squared (~100 sq.ft), and 11 listings with more than 6 car spots - all of these observations were removed to clean the data and ease processing.

 - On our first trial runs to fit models, it took over 10 minutes to run the `R` code on the remaining data on our machines, so we made the executive decision to randomly downsample the data to `r percent_to_keep*100`% of the total size before splitting the train/test set to 60%/40% of the remaining data. This cut down the processing time to be more reasonable.
 
##### Fitting models

```{r}
# for ease of calculating LOOCV RMSE
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# Here we will perform forward and backwards searches, favoring BIC
# The goal will be to generate a handful of models and compare them with one another

# starting points for forward searches
n = nrow(housing_trn) # for BIC
# #1 - start with no predictors, limit search to all predictors (additive)
start_forward_model_1 = lm(Price ~ 1, data = housing_trn)
forward_bic_model_1 = step(start_forward_model_1, scope = Price ~ (Rooms + Type + Method + Distance + Bathroom + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + Regionname + Propertycount), direction = "forward", k = log(n), trace = FALSE)

# #2 - start with no predictors, use log of response, limit search to all predictors (additive)
start_forward_model_2 = lm(log(Price) ~ 1, data = housing_trn)
forward_bic_model_2 = step(start_forward_model_2, scope = log(Price) ~ (Rooms + Bathroom + Car + Distance + Type + Regionname + Landsize + Method + BuildingArea + YearBuilt), direction = "forward", k = log(n), trace = FALSE)

# #3 - start with no predictors, use log of response, limit search to 2nd-order interactions of all predictors
start_forward_model_3 = lm(log(Price) ~ 1, data = housing_trn)
forward_bic_model_3 = step(start_forward_model_3, scope = log(Price) ~ (Rooms + Bathroom + Car + Distance + Type + Regionname + Landsize + Method + BuildingArea + YearBuilt)^2, direction = "forward", k = log(n), trace = FALSE)


# starting points for backward searches
# #1 - start with all predictors (additive)
start_backward_model_1 = lm(Price ~ (.), data = housing_trn)
backward_bic_model_1 = step(start_backward_model_1, direction = "backward", k = log(n), trace = FALSE)

# #2 - start with all 2nd-order interactions - remove CouncilArea since there are 33 levels to the factor
start_backward_model_2 = lm(Price ~ (.-CouncilArea)^2, data = housing_trn)
backward_bic_model_2 = step(start_backward_model_2, direction = "backward", k = log(n), trace = FALSE)

# #3 - add in polynomial terms with 2nd-order interactions- remove CouncilArea since there are 33 levels to the factor
start_backward_model_3 = lm(Price ~ (.-CouncilArea)^2 + I(Distance^2) + I(Rooms^2) + I(BuildingArea^2) + I(Propertycount^2), data = housing_trn)
backward_bic_model_3 = step(start_backward_model_3, direction = "backward", k = log(n), trace = FALSE)

# #4 - add in polynomial and log terms with 2nd-order interactions- remove CouncilArea since there are 33 levels to the factor
start_backward_model_4 = lm(log(Price) ~ (.-CouncilArea)^2 + I(Distance^2) + I(Rooms^2) + I(BuildingArea^2) + I(Propertycount^2), data = housing_trn)
backward_bic_model_4 = step(start_backward_model_4, direction = "backward", k = log(n), trace = FALSE)


# Manually manipulate the search results to see if we can find a better performing model with LOOCV

#Trying some quadratic expressions based on the key predictor [found by human analysis]

manual_model_1 = lm(log(Price) ~ Rooms + Type + Bathroom + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + I(Rooms^2) + I(Landsize^2) + I(BuildingArea^2), data = housing_trn)

manual_model_2 = lm(log(Price) ~ Rooms + Type + Distance + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + Propertycount + I(Propertycount^2) + I(Rooms^2) + I(Distance^2) + I(BuildingArea^2), data = housing_trn)

manual_model_3 = lm(log(Price) ~ log(I(1/Distance)) + Car + Rooms + log(Landsize) + log(Landsize):Rooms + Type + log(I(1/Distance)):Type + Method + log(Landsize):Method + YearBuilt + log(Landsize):YearBuilt, subset = (Distance > 0) & (Landsize > 0), data = housing_trn)

manual_model_4 = lm(formula = Price ~ Rooms + Type + Bathroom + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + I(Rooms^2) + I(Landsize^2) + I(BuildingArea^2), data = housing_trn)


# Model selection - pick best forward and backward models
# forward BIC search model LOOCV RMSE:
(fw1_loocv = calc_loocv_rmse(forward_bic_model_1))
(fw2_loocv = calc_loocv_rmse(forward_bic_model_2)) # log response
(fw3_loocv = calc_loocv_rmse(forward_bic_model_3)) # log response

# backward BIC search model LOOCV RMSE:
(bw1_loocv = calc_loocv_rmse(backward_bic_model_1))
(bw2_loocv = calc_loocv_rmse(backward_bic_model_2))
(bw3_loocv = calc_loocv_rmse(backward_bic_model_3))
(bw4_loocv = calc_loocv_rmse(backward_bic_model_4)) # log response

# manual tweaking LOOCV RMSE:
(man1_loocv = calc_loocv_rmse(manual_model_1)) # log response
(man2_loocv = calc_loocv_rmse(manual_model_2)) # log response
(man3_loocv = calc_loocv_rmse(manual_model_3)) # log response
(man4_loocv = calc_loocv_rmse(manual_model_4))

# select the best non-log and log transformed response models since they can't be directly
# compared using LOOCV RMSE (as units are different)
which.min(c(fw1_loocv, bw1_loocv, bw2_loocv, bw3_loocv, man4_loocv))
final_nonlog_model = backward_bic_model_3

which.min(c(fw2_loocv, fw3_loocv, bw4_loocv, man1_loocv, man2_loocv, man3_loocv))
final_log_model = backward_bic_model_4

# Compare using base RMSE on training set for the log and non-log models

# log model
# re-calculate residuals so they're using the same units as the non-log model
pred_vals = exp(predict(final_log_model, newdata = housing_trn))
(final_model_RMSE = sqrt(mean((pred_vals - housing_trn$Price)^2)))
(avg_pct_error1 = mean(abs((pred_vals - housing_trn$Price))/housing_trn$Price) * 100)

# non-log model
pred_vals2 = predict(final_nonlog_model, newdata = housing_trn)
(nonlog_model_RMSE = sqrt(mean((pred_vals2 - housing_trn$Price)^2)))
(avg_pct_error2 = mean(abs((pred_vals2 - housing_trn$Price))/housing_trn$Price) * 100)


# The non-log model predicts a house to have a negative price...combined with the higher RMSE we won't use this model
pred_vals2[pred_vals2 < 0]

# Thus the final winner is the log model (which has the lowest RMSE of the two on the test data): 
    # lm(formula = log(Price) ~ Rooms + Type + Method + Distance + 
    # Bathroom + Car + Landsize + BuildingArea + YearBuilt + Regionname + 
    # Propertycount + I(Distance^2) + I(BuildingArea^2) + I(Propertycount^2) + 
    # Rooms:Type + Rooms:Car + Rooms:BuildingArea + Type:Landsize + 
    # Type:YearBuilt + Distance:Car + Distance:Landsize + Distance:BuildingArea + 
    # Distance:Regionname + Landsize:BuildingArea + Landsize:Propertycount + 
    # BuildingArea:YearBuilt + BuildingArea:Regionname + Regionname:Propertycount, 
    # data = housing_trn)

# Checking the summary output for the winning model, we don't see any immediate non-factor interactions or variables that look like good candidates for removal (the non-significant coefficients are interactions with various dummy variables for factors that have other significant relationships with the response)
summary(backward_bic_model_4)
```

***

### Results

```{r}
options(scipen=999)
model_data = data.frame(row_labels=c("Forward Search 1", "Backward Search 1", "Backward Search 2", "Backward Search 3*", "Manual Selection 4", "Forward Search 2", "Forward Search 3", "Backward Search 4*", "Manual Selection 1", "Manual Selection 2", "Manual Selection 3"),
loocv_RMSE = c(fw1_loocv, bw1_loocv, bw2_loocv, bw3_loocv, man4_loocv, fw2_loocv, fw3_loocv, bw4_loocv, man1_loocv, man2_loocv, man3_loocv))
library(knitr)
kable(model_data, format = "markdown", col.names = c("Models", "Train LOOCV RMSE"))
```
 - Above we see the LOOCV RMSE values using the training dataset, along with the two models selected for final comparison - the lowest non-long LOOCV RMSE model, and the lowest log-response LOOCV RMSE model, as they cannot be directly compared in that fashion. After further checks on the test data shown in the `Methods` section, we determined the final model to be the log response contender:
 
 - Selected model: **lm(formula = log(Price) ~ Rooms + Type + Method + Distance + Bathroom + Car + Landsize + BuildingArea + YearBuilt + Regionname + Propertycount + I(Distance^2) + I(BuildingArea^2) + I(Propertycount^2) + Rooms:Type + Rooms:Car + Rooms:BuildingArea + Type:Landsize + Type:YearBuilt + Distance:Car + Distance:Landsize + Distance:BuildingArea + Distance:Regionname + Landsize:BuildingArea + Landsize:Propertycount + BuildingArea:YearBuilt + BuildingArea:Regionname + Regionname:Propertycount, data = housing_trn)**

```{r}
# plot actual vs. predicted price values with the selected model
plot(pred_vals ~ housing_trn$Price, xlab = "Actual Values ($)", ylab = "Predicted Values ($)", main = "Predicted vs. Actual Values (Selected Log Model)", col = "dodgerblue", cex = 1)
abline(0,1, col="darkorange", lwd = 1) # perfect response matches will be on this line

# plot actual vs. predicted price values with the non-log alternate model
plot(pred_vals2 ~ housing_trn$Price, xlab = "Actual Values ($)", ylab = "Predicted Values ($)", main = "Predicted vs. Actual Values (Non-log Model)", col = "dodgerblue", cex = 1)
abline(0,1, col="darkorange", lwd = 1) # perfect response matches will be on this line
```
 
 - Here we see the two Actual vs. Predicted price plots of the final two models in contention. While both plots seem to follow the general trend of the training data, we see more overestimates at the low end of the price spectrum, which is where we believe the model should be most accurate. In addition, we should bring up the one negative price prediction the non-log model produced (seen in the `Methods` section). As a price should never be negative (unless somehow you paid someone to take a property off your hands...), this hints at a flaw in the non-log response model. Thus we can be more confident in the log-response model as we do not see any negative prices (nor would be expect any). The final training RMSE of the selected model is `r final_model_RMSE`.

```{r}
# This analysis is secondary to the goal of prediciting housing values (vs. explaining), but still worthy of investigating in case we did want to use the model for explanation

# check the final two selected models (one log-response, other non-log)
library(lmtest)
qqnorm(resid(final_nonlog_model), main = "Normal Q-Q Plot, Final Non-Log Price Model", col = "darkgrey")
qqline(resid(final_nonlog_model), col = "dodgerblue", lwd = 2)
shapiro.test(resid(final_nonlog_model))
  
plot(fitted(final_nonlog_model), resid(final_nonlog_model), col = "grey", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Backward BIC Selected Non-Log Response Model")
abline(h = 0, col = "darkorange", lwd = 2)
bptest(final_nonlog_model)

qqnorm(resid(final_log_model), main = "Normal Q-Q Plot, Final Log Price Model", col = "darkgrey")
qqline(resid(final_log_model), col = "dodgerblue", lwd = 2)
shapiro.test(resid(final_log_model))
  
plot(fitted(final_log_model), resid(final_log_model), col = "grey", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Backward BIC Selected Log Response Model")
abline(h = 0, col = "darkorange", lwd = 2)
bptest(final_log_model)
```

 - We can see that the log-response model has a much better looking Normal Q-Q plot and Fitted vs. Residuals plot (though both still fail the Shapiro and Breusch-Pagan tests....which is ok if we are just using the model for prediction). This gives us even more confidence that the log model was the correct choice between the final two for prediction of housing prices, but it would be quite difficult (if not flawed) to use this model for explanation of relationships between predictors and the price response.

```{r}
# Training results table
model_data2 = data.frame(row_labels=c("Final model", "Runner-up"),
final_RMSE = c(final_model_RMSE, nonlog_model_RMSE),
final_avg_pct_error = c(avg_pct_error1, avg_pct_error2))
kable(model_data2, format = "markdown", col.names = c("Models", "Training RMSE", "Avg % Error"))

# Performance of the log model on the test data
pred_vals_final = exp(predict(final_log_model, newdata = housing_tst))
final_data_test_RMSE = sqrt(mean((pred_vals_final - housing_tst$Price)^2))
(avg_pct_error_final = mean(abs((pred_vals_final - housing_tst$Price))/housing_tst$Price) * 100)

# Final results table
model_data3 = data.frame(row_labels=c("Final model"),
final_RMSE = c(final_data_test_RMSE),
final_avg_pct_error = c(avg_pct_error_final))
kable(model_data3, format = "markdown", col.names = c("Model", "Test RMSE", "Avg % Error"))
```
 
 - As it is frowned upon to compare models (and make a selection between two) based on the test set, we decided to use the training data and select the model with the lowest regular RMSE (which also happened to have the lowest average percent error) to be our final model. The table above displays the final results of our selected model on the test set.


***

### Discussion

 - After utilizing several regression methods (forward and backwards search with multiple linear regression, dummy variables, interaction, transformations, and polynomials), along with model selection methods such as BIC and training set LOOCV RMSE, we arrived at our final model. Using several starting points for both forward and backwards search, along with using some manually manipulated selection of predictors (based off of the search results), we then compared all the resulting models with one another using LOOCV RMSE. We picked the lowest  `log(Price)` model LOOCV RMSE and the lowest non-log response model LOOCV RMSE, and as we could not directly compare these two RMSE (different units), we had to examine further selection criteria. We used the training set regular RMSE values of the two models, and selected the final log-response model as it had a lower RMSE (and also better model assumption plots, though still not ideal for using the model for explanation).
 
 - As we can see with `r length(coef(final_log_model))` coefficients in the model, it is quite complex. But as our stated goal was to *predict* housing prices, we feel that having a model which we can't use to make conclusions about relationships of predictors to the response is an acceptable tradeoff to increase price prediction accuracy. 
 
 - From the data is seems like the higher the price gets, the more variable the relationships with the predictors becomes. We can see that the model performs decently for the first half of the data (up to ~\$3 million) but seems to overestimate prices above that level. In the real world, we would be most confident using this model to predict prices below $2 million. In fact removing data above $3 million and performing the above analysis again would probably be a worthy endeavour to decrease the RMSE of the model.

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(ggmap)

# original data set
 melbourn_housing = read_csv("Melbourne_housing_FULL.csv", na = "NA")
 melbourn_housing = na.omit(melbourn_housing)
 melbourn_housing$Regionname = as.factor(melbourn_housing$Regionname)

# set color scale
library(RColorBrewer)
myColors = brewer.pal(8,"Set1")
names(myColors) = levels(melbourn_housing$Regionname)
colScale = scale_colour_manual(name = "Region",values = myColors)

melbourn_map = get_map(location = "melbourne", maptype = "watercolor", zoom = 9)
ggmap(melbourn_map) + geom_point(
  aes(x = Longtitude, y = Lattitude, colour = Regionname),
  alpha = 0.4,
  size = 2,
  data = melbourn_housing
  ) + colScale + guides(colour = guide_legend(override.aes = list(alpha = 1)))
```
 
 - In regards to generalizing this model to other markets, `Regionname` would probably have to be recreated, and may end up behaving differently depending how the area splits up their regions. Measurement units would also need to be standardized if a US market was used, as SI units were used in this dataset. Above the latitude and longitude of all of the properties from the data set are plotted on top of a map of Melbourne - not every city would be able to be broken up as such (especially being a port next to the ocean), so this model may not generalize very well, but the methods could be followed for any similar datasets.
 
***

### Appendix
```{r}
# Summary output of all models compared before final selection:
summary(forward_bic_model_1)
summary(forward_bic_model_2)
summary(forward_bic_model_3)
summary(backward_bic_model_1)
summary(backward_bic_model_2)
summary(backward_bic_model_3)
summary(backward_bic_model_4)
summary(manual_model_1)
summary(manual_model_2)
summary(manual_model_3)
```

